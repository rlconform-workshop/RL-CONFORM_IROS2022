<!DOCTYPE HTML>
<!--
Hyperspace by HTML5 UP
html5up.net | @ajlkn
Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
<head>
	<title>RL-CONFORM 2022</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">   
	<link rel="stylesheet" href="assets/css/main.css" /> 
	<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
</head>
<body class="is-preload">

	<!-- Sidebar -->
	<section id="sidebar"> 
	<div id="nav-inner" class="inner">
	   <nav>
	  <a href="#intro">About</a>
	
	  	  <!-- <a href="#program">Schedule</a> -->
	      <a href="#program">Program</a>
		  <!-- <a href="#shortpaper2">Short Papers II</a> -->
		  <a href="#two">Invited Speakers</a>
		  <a href="#panelists">Invited Panelists</a>


		<a href="#cfp">Call for Papers</a>
		<a href="#organizers">Organizers</a>
		<a href="#rlconform21">Previous Editions</a>
		<a href="#socials">Social Media</a>
	  
		</nav>
		</div>


	</section> 

	<!-- Wrapper -->
	<div id="wrapper">

		<!-- Intro -->
		<section id="intro" class="wrapper style1 fullscreen fade-up" style="width:100%;">
			<div class="inner" style="padding: 5em 6vw 5em 11vw;">
				<img src="images/RLCONFORM-website.png" style="float:right; cursor:pointer; cursor:hand; border:0; margin: -4vw 8vw 3vw 3vw" width="35%;" alt="RLCONFORM logo">
				<h1>2nd RL-CONFORM Workshop at IROS'22 </h1>
				<h2>Reinforcement Learning meets HRI, Control, and Formal Methods </h2>
				<!-- <h3>co-located with IROS'22</h3> -->

				<h3 style="color:rgb(247, 154, 119);">
					Thank you to all speakers, panelists, short paper presenters, and participants for making the 2nd RL-CONFORM a success!
					We have now updated our website with recordings of our keynote sessions linked in the <a href="#program">Program</a>.

				
					<!-- <a style="color: blue !important;" href="https://kth-se.zoom.us/j/66672610238?pwd=a2lNbTJNbWVJaDZKb2lQVjVKRnpvUT09">Go to Zoom</a> </h3>
				<h3 style="color:rgb(247, 154, 119);">
					You can send us your feedback, comments, and questions using Menti.com with the code 8762 7108. 
				<a style="color: blue !important;" href="https://menti.com">Go to Menti</a>  -->
					
				</h3>
				

				

				<p>&nbsp;</p>

				<p> Reinforcement learning (RL) has shown remarkable achievements in applications ranging from autonomous driving, object manipulation, or beating best players in complex board-games. 
					Different communities, including RL, human-robot interaction (HRI), control, and formal methods (FM), have proposed multiple techniques to increase safety, transparency, and robustness of RL.
					However, elementary problems of RL remain open: exploratory and learned policies may cause unsafe situations, lack task-robustness, or be unstable. 
					By satisfactorily addressing these problems, RL research will have long-lasting impact and see breakthroughs on real physical systems and in human-centered environments. 
					As an example, a collaborative mobile manipulator needs to be robust and verifiably safe around humans. 
				This requires an integrated approach with RL to learn optimal policies for complex manipulation tasks, control techniques to ensure stability of the system, FM techniques to provide formal guarantees to ensure safety, and techniques from human-robot interaction to learn from and interact with humans.</p>
				<p>

					The aim of this multidisciplinary workshop is to bring these communities together to: 				</p>
					<ol type="i">
						<li>Identify key challenges and opportunities related to safe and robust exploration, formal safety and stability guarantees of control systems, safety in physical human-robot collaborative systems;</li>
						<li>Provide unique insights into how these challenges depend on the application, desired system properties, and complexity of the environment;</li>
						<li>Propose new and debate existing approaches to ensure desired properties of learned policies in a wide range of domains;</li>
						<li>Discuss existing and new benchmarks to accelerate safe and robust RL research; </li>
						<li>Disseminate the outcomes of the workshop and publish the results as a perspectives article in one of the major robotics journals.</li>
					</ol>
					<p>The themes of the workshop include but are not limited to RL and control theory, RL and Human-Robot Interaction, RL and Formal Methods, and benchmarking of RL. 
					</p>

			</p>
		</div>
	


	<!-- One -->
	<section id="one" class="wrapper style3 fade-up">

		<div class="inner" style="padding: 17vw 2vw 2vw 10vw;">
			<h2 id="program">Program (all times are JST) - October 23, 2022. 
			</h2>
			<p>&nbsp;</p>
			<!-- <span style="font-weight: 400;"> The RL-CONFORM workshop will be held via The RL-CONFORM workshop will be held via <a href="https://kth-se.zoom.us/j/61682931249"> Zoom </a> and <a href="https://iros2021.gcon.me/page/home"> gCon </a> </span> -->


			<table class="table table-striped">
				<tbody>
					<tr>
						<th> Time (JST)</th>
					</tr>
					<tr>
						<td style="width:25%;">	09:00 am - 09:05 am        </td><td></td><td>  Organizers <br> <b> Welcome!   </b> </td>
					</tr>

					<tr>
						<td>	09:05 am - 09:25 am        </td><td></td><td>  Invited Speaker: <a href="#niekum">Scott Niekum </a> <br> <b> 20-minute talk  </b><br> <b><a href="https://youtu.be/ezoz3J8QZvM">Recording</a> </b> </td>
					</tr>

					<tr>
						<td>	09:25 am - 09:45 am        </td><td></td><td>  Invited Speaker: <a href="#chalvatzaki">Georgia Chalvatzaki</a> <br>
							<b> 20-minute talk  </b> <br> <b> <a href="https://youtu.be/tNtGSoxuVkc">Recording</a> </b> </td>
						</tr>

					<tr>
						<td>	09:45 am - 10:30 am        </td><td></td><td>   Panel Discussion I <br> <b> 45-minute panel session </b> </td>
					</tr>

					<tr>
						<td>	10:30 am - 10:50 am        </td><td></td><td>  Coffee Break </td>
					</tr>

					<tr>
						<td>	10:50 am - 11:10 am        </td><td></td><td>  Invited Speaker:  <a href="#bohg">Jeanette Bohg</a> <br>
							<b> 20-minute talk  </b> <br> <b> <a href="https://youtu.be/9qz3I6G2T00">Recording</a> </b> </td>
						</tr>

						<tr>
							<td>	11:10 am - 11:30 am        </td><td></td><td>  Invited Speaker:  <a href="#hayes">Bradley Hayes</a>  <br>
								<b> 20-minute talk  </b>   <br> <b> <a href="https://youtu.be/FoXKNUUiHvI">Recording</a> </b> </td>
							</tr>

					<tr>
						<td>	11:30 am - 12:00 pm        </td><td></td><td>  <a href="#shortpaper1">Short Papers I </a> <br>
							<b> 4-minute talks  </b> </td>
						</tr>

						<tr>
							<td>	12:00 pm - 01:30 pm        </td><td></td><td>  Lunch Break </td>
						</tr>

						<tr>
							<td>	01:30 pm - 01:50 pm        </td><td></td><td>  Invited Speaker:  <a href="#osa">Takayuki Osa</a> <br> <b> 20-minute talk  </b> <br> <b> <a href="https://youtu.be/p8vYZSun1jQ">Recording</a></b></td>
						</tr>

						<tr>
							<td>	01:50 pm - 02:10 pm        </td><td></td><td>  Invited Speaker:  <a href="#ramos">Fabio Ramos</a> <br>
								<b> 20-minute talk  </b> <br> <b><a href="https://youtu.be/dU-cdN_q5qU">Recording</a> </b></td>
							</tr>

							<tr>
								<td>	02:10 pm - 02:20 pm        </td><td></td><td>  Coffee Break </td>
							</tr>

							<tr>
								<td>	02:20 pm - 03:05 pm        </td><td></td><td>  Panel Discussion II <br>
									<b> 45-minute panel  </b> </td>
								</tr>

								<tr>
									<td>	03:05 pm - 03:25 pm        </td><td></td><td>  Coffee break </td>
								</tr>

								<tr>
									<td>	03:25 pm - 03:45 pm        </td><td></td><td>  Invited Speaker:  <a href="#kressgazit">Hadas Kress-Gazit</a><br>
										<b> 20-minute talk  <b> <br> <b><a href="https://youtu.be/LfrIqX-x2p8">Recording</a> </b></td>
									</tr>

									<tr>
										<td>	03:45 pm - 04:05 pm        </td><td></td><td>  Invited Speaker:  <a href="#jansen">Nils Jansen</a> <br>
											<b> 20-minute talk  </b> <br> <b><a href="https://youtu.be/9mTpfQOy1ew">Recording</a></b> </td>
										</tr>


										<tr>
											<td>	04:15 pm - 04:45 pm        </td><td></td><td>  <a href="#shortpaper2">Short Papers II</a>  <br>
												<b> 4-minute talks  </b> </td>
											</tr>


											<tr>
												<td>	
												04:45 pm - 05:00 pm    </td><td></td><td>   Organizers <br>  <b>    Closing Remarks </b>
												</td>
											</tr>
										</tbody>
									</table>
				<p>&nbsp;</p>
		
		</section>

		<section id="one" class="wrapper style2 fade-up">
			<div class="inner" style="padding: 17vw 2vw 2vw 10vw;">
			<table cellpadding=10 cellspacing=0> 
			<tbody> 
			<h2 id="shortpaper1">Short paper presentations I (morning session) </h2>
 			<p>PDFs of the papers will be added to the website shortly</p>

			<table class="table table-striped">
				<tbody>
					<tr>
						<th> Time (JST)</th>
					</tr>
					<tr>
						<td style="width:20%;">	11:30-11:36 am     </td><td></td><td>Tung Nguyen and Johane Takeuchi. <br> <b>Utilization of domain knowledge to improve POMDP belief estimation. <a href="https://drive.google.com/file/d/1luml6YZJ3l_b8TGgEFGiRUSw_VI4L8eg/view?usp=sharing"> (paper) </a></b> </td> 
					</tr>

					<tr>
						<td>	11:36-11:42 am &nbsp;       </td><td></td><td>Francisco Cruz, Adam Bignold, Hung Son Nguyen, Richard Dazeley and Peter Vamplew. <br> <b>Broad-persistent advice for interactive reinforcement learning scenarios. <a href="https://arxiv.org/abs/2210.05187"> (paper) </a></b> </td>
					</tr>

					<tr>
						<td>	11:42-11:48 am &nbsp;       </td><td></td><td>Stefano Massaroli, Michael Poli, Ren Komatsu, Alessandro Moro, Atsushi Yamashita and Hajime Asama.<br> <b>Model-Based Policies in Continuous Time, States and Actions: Surrogate Models and Gradient Estimation.</b> </td>
					</tr>

					<tr>
						<td>	11:48-11:54 am &nbsp;       </td><td></td><td>Finn Rietz, Erik Schaffernicht, Todor Stoyanov and Johannes Andreas Stork.<br> <b>Towards Task-Prioritized Policy Composition. <a href="https://arxiv.org/pdf/2209.09536.pdf"> (paper) </a>  </b> </td>
					</tr>

					<tr>
						<td>	11:54-12:00 pm &nbsp;       </td><td></td><td>Eugene Lim and Harold Soh<br> <b>Observed Adversaries in Deep Reinforcement Learning. <a href="https://github.com/clear-nus/observersary/raw/main/papers/iros-observed-adversary.pdf"> (paper) </a></b> </td>
					</tr>


				</tbody>

			</table>
			
			<p>&nbsp;</p>

			<h2 id="shortpaper2">Short paper presentations II (afternoon session) </h2>
			<p>PDFs of the papers will be added to the website shortly</p>



			<table class="table table-striped">
				<tbody>
					<tr>
						<th> Time (JST)</th>
					</tr>
					<tr>
						<td style="width:20%;">	04:15-04:21 pm  &nbsp;      </td><td></td><td>Mudit Verma, Ayush Kharkwal and Subbarao Kambhampati.<br> <b>Advice Conformance Verification by Reinforcement Learning agents for Human-in-the-Loop.  <a href="https://arxiv.org/abs/2210.03455"> (paper) </a><a href="https://www.youtube.com/watch?v=XSyN_zYslI0"> (video) </a> </b> </td> 
					</tr>

					<tr>
						<td>	04:21-04:27 pm  &nbsp;      </td><td></td><td>Mudit Verma and Katherine Metcalf<br> <b>Symbol Guided Hindsight Priors for Reward Learning from Human Preferences. <a href="https://arxiv.org/abs/2210.09151"> (paper) </a><a href="https://www.youtube.com/watch?v=gzwyimBRzzM"> (video) </a></b> </td> 
					</tr>

					<tr>
						<td>	04:27-04:33 pm  &nbsp;      </td><td></td><td>Quantao Yang, Johannes A. Stork and Todor Stoyanov.<br> <b>Transferring Knowledge for Reinforcement Learning in Contact-Rich Manipulation. <a href="https://arxiv.org/abs/2210.02891"> (paper) </a></b> </td>
					</tr>

					<tr>
						<td>	04:33-04:39 pm  &nbsp;      </td><td></td><td>Xavier Weiss, Saeed Mohammadi, Parag Khanna, Mohammad Reza Hesamzadeh and Lars Nordström. <br> <b>Learn to Run Power Network (L2RPN) Safely with Deep Reinforcement Learning. <a href="https://youtu.be/ev8eJ8zIHQE"> (video) </a></b> </td> 
					</tr>

					<tr>
						<td>	04:39-04:45 pm  &nbsp;      </td><td></td><td>Alexander Dürr, Volker Krueger and Elin Anna Topp. <br> <b>Towards a comprehensive study to identify the relevant I/O to support or complement Reinforcement Learning.</b> </td> 
					</tr>


			</tbody>
			</table>
		</div>
	</section>	

	<section id="speakers" class="wrapper style3 fade-up">

			<div class="inner">
						<p>&nbsp;</p>
						<!-- 	<h2 style="border-bottom: 5px solid #7a0000;"> -->
							<h2>Invited Speakers</h2>

							<section id="ramos">
							<div class="content">
								<div class="inner">
									<h3><a href="https://fabioramos.github.io/Home.html"> Fabio Ramos</a>, NVIDIA and University of Sydney, AU.</h3>
									<img id="headshot" src="images/FabioRamosTalk.png" width="25%" style="float:right;" alt="Headshot of Fabio Ramos" />
									<p> Title: Leveraging Differentiable Simulation for Reinforcement Learning and Bayesian Domain Randomization <p>
									
									<p> Abstract: Differentiable simulation can play a key role in scaling reinforcement learning to higher dimensional state and action spaces, while, at the same time, leveraging recent probabilistic inference methods for Bayesian domain randomization. In this talk, I will discuss advantages and disadvantages of differentiable simulation and connect it with two methods that use differentiability to speed up Bayesian inference, stochastic gradient Langevin dynamics and Stein Variational Gradient Descent. Our resulting Bayesian domain randomization approach can quickly produce posterior distributions over simulation parameters given real state-action trajectories, leading to robust controllers and policies. I will show examples in legged locomotion, robotics manipulation, and robotics cutting. </p>
									<p> Bio: 
										Fabio is a Principal Research Scientist at NVIDIA, and Professor in machine learning and robotics at the School of Computer Science, University of Sydney. Before, Fabio was a co-Director of the Centre for Translational Data Science,  and previously an Australian Research Council (ARC) Research Fellow at the Australian Centre for Field Robotics. Fabio's research is focused on modelling and understanding uncertainty for prediction and decision making tasks, and includes Bayesian statistics, data fusion, anomaly detection, and reinforcement learning. Over the last ten years Fabio has applied these techniques to robotics, mining and exploration, environment monitoring, and neuroscience.

									</p>

								</div>
							</div>
							</section>
							<hr style="border: 1px solid white">


							<section id="chalvatzaki">
							<div class="content">
								<div class="inner">
									<h3><a href="https://www.ias.informatik.tu-darmstadt.de/Team/GeorgiaChalvatzaki">Georgia Chalvatzaki</a>, TU Darmstadt, DE.</h3>
									<img id="headshot" src="images/GeorgiaChalvatzakiTalk.png" width="25%" style="float:right"; alt="Headshot of Georgia Chalvatzaki" >
									<p> Title: Learning adaptive and safe human-centric robot behaviors</p>
									<p> Abstract:  The need for intelligent and safe robotic assistants is more urgent than ever in hospitals, nursing homes, caring facilities, etc. In this talk, I will discuss problems I have addressed over the years regarding human-centered robotic assistants that learn to adapt to humans, ranging in applications regarding elderly support and child-robot interaction. I will cover methods for human behavior understanding and intention prediction, and explain how structured models and modern deep learning methods allow effective, adaptive physical human-robot interaction behaviors. I will finish my talk with our recent work on modeling human and object manifolds to use them as differentiable constraints in safe reinforcement learning methods of robot behaviors, particularly for learning safe robot interactions in the presence of humans.</p>

									<p> Bio: Dr. Georgia Chalvatzaki is an Assistant Professor and the research leader of the intelligent robotic systems for assistance (iROSA) group at TU Darmstadt and a member of Hessian.AI. She received the Emmy Noether grant from the German Research Foundation (DFG) in 2021. In iROSA, her team researches the topic of "Robot Learning of Mobile Manipulation for Intelligent Assistance," investigating novel methods for combined planning and learning to enable mobile manipulator robots to solve complex tasks in house-like environments with the human-in-the-loop of the interaction process. She is co-chair of the IEEE RAS technical committee of Mobile Manipulation and co-chair of the IEEE RAS Women in Engineering Committee.

									</p>
								</div>
							</div>
							</section>
							<hr style="border: 1px solid white">


							<section id="osa">
							<div class="content">
								<div class="inner">
									<h3><a href="https://takaosa.github.io/about.html">Takayuki Osa</a>, University of Tokyo, JP.</h3>
									<img id="headshot"src="images/TakayukiOsaTalk.png" width="25%" style="float:right"; alt="Headshot of Takayuki Osa" >
									<p> Title: What should we learn in a robot-learning system? </p>
									<p> Abstract: To deploy a robot-learning system in the real world, it is essential to ensure the safety of the system. In this talk, we discuss what to learn in a robot-learning system and how to learn it to make the system reliable. As an approach towards safe robot-learning systems, we introduce methods for learning diverse solutions in motion planning and reinforcement learning. Through case studies, we demonstrate that the diversity of solutions offers the flexibility and reliability to a robot-learning system.   </p>

									<p> Bio: Takayuki Osa is an associate professor at the University of Tokyo and a visiting researcher at RIKEN Center for Advanced Intelligence Project (AIP). Before joining UTokyo in June 2022, he was an associate professor at Kyushu Institute of Technology. From April 2017 to February 2019, he was a project assistant professor at the University of Tokyo. Takayuki Osa received his Ph.D. in Engineering from the University of Tokyo in 2015. From 2015 to 2017, he worked with Jan Peters and Gerhard Neumann at TU Darmstadt in Germany as a post-doctoral researcher. Industrial partners include Honda Motor Co., Ltd., Komatsu Ltd.

									</p>
								</div>
							</div>
							</section>
							<hr style="border: 1px solid white">



							<section id="kressgazit">
							<div class="content">
								<div class="inner">
									<h3><a href="https://research.cornell.edu/researchers/hadas-kress-gazit">Hadas Kress-Gazit</a>, Cornell University, US.</h3>
									<img id="headshot"src="images/HadasKressGazitTalk.png" width="25%" style="float:right"; alt="Headshot of Hadas Kress-Gazit" >
									<p> Title: Skills and composition: a wish list for RL</p>
									<p> Abstract: Given a set of robot skills, one can compose them to achieve complex robot behaviors, be it through classical planning, or synthesis from temporal logic requirements. But where do these skills come from? How should we learn them? How should we modify them if needed? In this talk, I will give my own personal wish list for properties of RL and will be excited to learn whether my ideal algorithm is already a reality.  </p>

									<p> Bio: Hadas Kress-Gazit is a Professor at the Sibley School of Mechanical and Aerospace Engineering at Cornell University. She received her Ph.D. in Electrical and Systems Engineering from the University of Pennsylvania in 2008 and has been at Cornell since 2009. Her research focuses on formal methods for robotics and automation and more specifically on creating verifiable robot controllers for complex high-level tasks using logic, verification, synthesis, hybrid systems theory and computational linguistics. She received an NSF CAREER award in 2010, a DARPA Young Faculty Award in 2012 and the Fiona Ip Li '78 and Donald Li '75 Excellence in teaching award in 2013.
									</p>
								</div>
							</div>
							</section>
							<hr style="border: 1px solid white">


							<section id="hayes">
							<div class="content">
								<div class="inner">
									<h3><a href="https://www.colorado.edu/cs/bradley-hayes"> Bradley Hayes</a>, University of Colorado Boulder, US.</h3>
									<img id="headshot"src="images/BradleyHayesTalk.png" width="25%" style="float:right"; alt="Headshot of Bradley Hayes" />
									<p> Title: Making Capable Human-Robot Teams through Reinforcement Learning and Multimodal Communication</p>
									
									<p> Abstract: Robots capable of collaborating with or otherwise assisting humans can bring transformative changes to the way we live and work. So where are these robots that we've been told about for the past 20 years? Deployments into human-populated environments and onto human teams remain largely infeasible due to the challenge of ensuring that robots are simultaneously proficient at task execution and at maintaining shared situational awareness with the humans around them. In this talk I will present my group's recent work toward overcoming these challenges via an interactive, communicative approach: blending novel techniques for imitation learning and autonomous coaching to realize autonomous systems that are able to transparently learn from and adapt to operation with and around humans, sharing their knowledge to improve collective situational awareness as well as individual or team performance. </p>
									<p> Bio: 
										Bradley Hayes is an Assistant Professor of Computer Science at the University of Colorado Boulder, where he directs the Collaborative AI and Robotics (CAIRO) Lab. Brad's research develops techniques to create and validate autonomous systems that learn from, teach, and collaborate with humans to improve mutual understanding, efficiency, safety, and capability at scale. His work combines novel approaches at the intersection of machine learning, cognitive science, and explainable artificial intelligence, toward making human-autonomy teams both more capable and more powerful than the sums of their parts. 

									</p>

								</div>
							</div>
							</section>

							<hr style="border: 1px solid white">

							<section id="niekum">
							<div class="content">
								<div class="inner">
									<h3> <a href="https://people.cs.umass.edu/~sniekum/">Scott Niekum</a>, University of Massachusetts Amherst, US.</h3>
									<img id="headshot"src="images/ScottNiekumTalk.png" width="25%" style="float:right"; alt="Headshot of Scott Niekum" >
									<p> Title: The Role of Guarantees in Value Alignment </p>
									<p> Abstract: As AI systems have become increasingly competent, value alignment -- ensuring that the goals and/or behaviors of AI systems align with human values -- has become a popular buzzword in the AI research community, though it's exact technical meaning is often unclear.  Perhaps the single most important distinction across value alignment methods is whether they provide performance guarantees of any form, suggesting several critical questions that the AI research community must address: Are guarantees integral to the core concept of value alignment? What types of guarantees are even possible in practice? And does value alignment without guarantees amount to anything more than a marketing strategy?  </p>
									<p> Bio: Scott is an Associate Professor and the director of the Safe, Confident, and Aligned Learning + Robotics Lab (SCALAR) in the College of Information and Computer Sciences at The University of Massachusetts Amherst. Scott is also a core member of the interdepartmental UMass robotics group, as well as an Adjunct Professor at the University of Texas at Austin. </p>
								</div>
							</div>
							</section>
							<hr style="border: 1px solid white">
														
							<section id="bohg">
							<div class="content">
								<div class="inner">
									<h3><a href="https://web.stanford.edu/~bohg/">Jeanette Bohg</a>, Stanford University, US.</h3>
									<img id="headshot"src="images/JeanetteBoghTalk.png" width="25%" style="float:right"; alt="Headshot of Jeanette Bohg" >
									<p> Title: Plan for any task: Resolving action dependencies in sequential manipulation task with learned manipulation primitives </p>
									<p> Abstract: Advances in robotic skill acquisition have made it possible to build general-purpose libraries of primitive skills for downstream manipulation tasks. However, naively executing these learned primitives one after the other is unlikely to succeed without accounting for dependencies between actions prevalent in long-horizon plans. In this talk, I will present a scalable framework for training manipulation primitives and coordinating their geometric dependencies at plan-time to efficiently solve long-horizon tasks never seen by any primitive during training. Based on the notion that Q-functions encode a measure of action feasibility, we formulate motion planning as a maximization problem over the expected success of each individual primitive in the plan, which we estimate by the product of their Q-values. Our experiments indicate that this objective function approximates ground truth plan feasibility and, when used as a planning objective, reduces myopic behavior and thereby promotes task success. We further demonstrate how our approach  can be used for task and motion planning by estimating the geometric feasibility of candidate action sequences provided by a task planner. We evaluate our approach in simulation and on a real robot.  </p>

									<p> Bio: Jeanette is a Professor for Robotics at Stanford University and director of the Interactive Perception and Robot Learning Lab. In general, Jeanette's research explores two questions: What are the underlying principles of robust sensorimotor coordination in humans, and how we can implement them on robots? Research on this topic has to necessarily be at the intersection of Robotics, Machine Learning and Computer Vision.

									</p>
								</div>
							</div>
							</section>
							<hr style="border: 1px solid white">

							<section id="jansen">
							<div class="content">
								<div class="inner">
									<h3><a href="http://www.cs.ru.nl/personal/nilsjansen/">Nils Jansen</a>, Radboud University, NL.</h3>
									<img id="headshot"src="images/NilsJansenTalk.png" width="25%" style="float:right"; alt="Headshot of Nils Jansen" >
									<p> Title: Safe Reinforcement Learning under Partial Observability </p>
									<p> Abstract: Safe exploration is a common problem in reinforcement learning (RL) that aims to prevent agents from making disastrous decisions while exploring their environment. A family of approaches to this problem assume domain knowledge in the form of a (partial) model of this environment to decide upon the safety of an action. A so-called shield forces the RL agent to select only safe actions. However, for adoption in various applications, one must look beyond enforcing safety and also ensure the applicability of RL with good performance. We discuss the applicability of shields via a tight integration with state-of-the-art deep RL, and provide an extensive, empirical study in challenging, sparse-reward environments under partial observability. We show that a carefully integrated shield ensures safety and can improve the convergence rate and final performance of RL agents. We furthermore show that a shield can be used to bootstrap state-of-the-art RL agents: they remain safe after initial learning in a shielded setting, allowing us to disable a potentially too conservative shield eventually.  </p>

									<p> Bio: Nils is an associate professor with the Department of Software Science (SWS) at Radboud University Nijmegen in the Netherlands. Nils a member of the ELLIS society. 
									
									</p>
								</div>
							</div>
							</section>
							<hr style="border: 1px solid white">
							<p>&nbsp;</p>
						</div>
					</section>
				</section>

	<section id="panelists" class="wrapper style2 fade-up">

	<div class="inner" style="padding: 17vw 2vw 2vw 10vw;">
	<h2>Invited Panelists</h2>
	
	<p style="color:white;">There will be two interactive panel sessions, one on <i>Principles and understanding of RL algorithms and models</i> and one on <i>Benchmarks, implementation, and accelerating RL research.</i> </p>
	<p style="color:white;">We, the workshop organizers, prioritize and strive to improve gender equity, diversity, and inclusion in our workshop and to our best ability plan our sessions accordingly. In this year's hybrid edition, our actions to accommodate time zones of our panelists might affect panel composition. </p>

	<p>&nbsp;</p>

	<h3>Benchmarks, implementation, and accelerating RL research <br> (9:45 - 10:30 am JST)</h3>
	<div class="card-group">
		<div class="card text-dark">
			<img class="card-img-top" style="object-position: 0 0%;" src="./images/TescaFitzgerald.jpg" alt="Headshot of Tesca Fitzgerald">
			<div class="card-body">
				<!-- <h5 class="card-title" style="color:black;">Talk Title TBD</h5> -->
				<p class="card-text"><a href="https://www.tescafitzgerald.com">Tesca Fitzgerald</a> is an Assistant Professor at Yale University, US.</p>

			</div>
		</div>
	
		<div class="card text-dark">
			<img class="card-img-top" src="./images/LukasBrunke.jpg" alt="Headshot of Lukas Brunke">
			<div class="card-body">
				<p class="card-text"><a href="https://www.lukasbrunke.com/">Lukas Brunke</a> is a PhD student at the University of Toronto Institute for Aerospace Studies, CA.</p>
			</div>
		</div>

		<div class="card text-dark">
			<img class="card-img-top" style="object-position: 0 0%;" src="./images/NathanFulton.jpg" alt="Headshot of Nathan Fulton ">
			<div class="card-body">
				<!-- <h5 class="card-title" style="color:black;">Talk Title TBD</h5> -->
				<p class="card-text"><a href="https://nfulton.org">Nathan Fulton</a> is a Senior Applied Scientist at Amazon Web Services, US.</p>

			</div>
		</div>
		<div class="card text-dark">
					<img class="card-img-top" src="./images/ElaineShort.jpg" alt="Headshot of Elaine Schaertl Short">
					<div class="card-body">
						<p class="card-text"><a href="https://engineering.tufts.edu/cs/people/faculty/elaine-short">Elaine Schaertl Short</a> is an Assistant Professor in the Tufts University Department of Computer Science, US.</p>
					</div>
				</div>
			</div>


	<!-- SECOND ROW -->		
	<p>&nbsp;</p>
	<h3>Principles and understanding of RL algorithms and models <br> (2:20 - 3:05pm JST)</h3>
	<div class="card-group">
		<div class="card text-dark">
			<img class="card-img-top" src="./images/JensKober.jpg" alt="Headshot of Jens Kober">
			<div class="card-body">
				<p class="card-text"><a href="http://www.jenskober.de">Jens Kober</a> is an Associate Professor at TU Delft, NL.</p>
			</div>
		</div>
		<div class="card text-dark">
			<img class="card-img-top" src="./images/HaroldSoh.jpg" alt="Headshot of Harold Soh">
			<div class="card-body">
				<p class="card-text"><a href="https://haroldsoh.github.io/">Harold Soh</a> is an Assistant Professor at the National University of Singapore, SG.</p>
			</div>
		</div>

		
				    <div class="card text-dark">
				    <img class="card-img-top" src="./images/Takamitsu.jpg" alt="Headshot of Takamitsu Matsubara ">
				    <div class="card-body">
				      <p class="card-text"><a href="https://sites.google.com/view/naist-robot-learning-en?authuser=0">Takamitsu Matsubara</a> is professor at the Nara Institute of Science and Technology and a visiting researcher at the ATR Computational Neuroscience Laboratories, Kyoto, Japan.</p>
				    </div>
				    </div>

				     

				    <div class="card text-dark">
			<img class="card-img-top" src="./images/MohanSridharan.jpg" alt="">
			<div class="card-body">
				<p class="card-text"><a href="">Mohan Sridharan</a> reader in Cognitive Robot Systems in the School of Computer Science at the University of Birmingham, UK.</p>
			</div>
		</div>


		</div>
			</div>
	</section>

	<section id="cfp" class="wrapper style3 fade-up">
	<div class="inner">
<!-- 		<p>&nbsp;</p>
 -->
		<h2>Call for Papers</h2>


		<p> We invite extended 2-4 page abstract submissions of recent works, preliminary work with open questions is very welcome, related to the theme of the workshop. All accepted abstracts will be part of a short paper presentation session held during the workshop, where the authors will have the opportunity to present their lines of work in a 5 minutes presentation, followed by a 3-minutes live Q&A session. This is a non-archival venue: there will be no formal proceedings, but we encourage the authors to publish their extended abstracts on arXiv (where the link will be placed on the workshop’s website). Abstracts may be submitted to other venues in the future. </p>

		<p>Based on the target areas and the discussions during our RL-CONFORM workshop at last year’s IROS, topics of interest include but are not limited to:  

			<ul>
				<li>Data-efficiency, sim-to-real gap, and guided exploration in RL;</li>
				<li>Safety guarantees, shielding, invariant sets, and online verification; </li>
				<li>Query sample-efficiency, human-robot interaction, learning from demonstration, and human feedback;</li>
				<li>Existing and new benchmarks to accelerate safe and robust RL research.</li>

			</ul>
		</p>


		<h3>Important workshop details</h3>

		<p> 
			<ul>
				<li>When: October 23, 2022.</li>
				<li>Where: Hybrid event co-located with IROS 2022 in Kyoto, Japan, and over Zoom.</li>
				<!-- <li>Where: <a href="https://kth-se.zoom.us/j/61682931249"> Zoom </a> and <a href="https://iros2021.gcon.me/page/home"> gCon </a></li> -->
<!-- 								  <li>Website: <a href="https://rlconform-workshop.github.io/">RL-CONFORM workshop 2022</a></li>
-->							  <li style="text-decoration-line: line-through;"> Submission deadline: September 3, 2022 (AoE)</li>
<li style="text-decoration-line: line-through;">Notification of acceptance: September 15, 2022</li>
<li>Submission format: 2-4 page abstracts (excl. references) of original, possibly ongoing research. Papers should be formatted in the IROS 2022 style guidelines, more information can be found at <a href="https://www.iros2022.org/call-for-papers"> IROS Call for Papers</a>.</li>
<li>To submit your work visit: <a href="https://easychair.org/conferences/?conf=rlconform2022">Easychair submission website</a> </li>
<li>Contact: <a href="mailto:rlconform2022@easychair.org"> rlconform2022@easychair.org</a> </li>
</ul>
</p>
</div>

</section>


<section id="organizers" class="wrapper style2 fade-up">
	<div class="inner">
		<!-- <p>&nbsp;</p> -->

		<h2>Organizing Committee and Support</h2>

		<h3>Program Chairs </h3>
		<div class="card-group">
			<div class="card text-dark">
				<img class="card-img-top" src="./images/profile_chris.jpg" alt="Headshot of Chris Pek">
				<div class="card-body">
					<p class="card-text" style="color:black"><a href="https://www.kth.se/profile/pek2">Christian Pek</a>, KTH Royal Institute of Technology, Sweden.</p>
				</div>
			</div>
			<div class="card text-dark">
				<img class="card-img-top" src="./images/profile_alexis.jpg" alt="Headshot of Alexis Linard">
				<div class="card-body">
					<p class="card-text" style="color:black"><a href="https://www.kth.se/profile/linard">Alexis Linard</a>, KTH Royal Institute of Technology, Sweden.</p>
				</div>
			</div>
			<div class="card text-dark">
				<img class="card-img-top" src="./images/profile_sanne.jpg" alt="Headshot of Sanne van Waveren">
				<div class="card-body">
					<p class="card-text"><a href="https://www.kth.se/profile/sannevw">Sanne van Waveren</a>, KTH Royal Institute of Technology, Sweden.</p>
				</div>
			</div>
			<div class="card text-dark">
				<img class="card-img-top" src="./images/profile_hang.jpg" alt="Headshot of Hang Yin">
				<div class="card-body">
					<p class="card-text" style="color:black"><a href="https://www.kth.se/profile/hyin">Hang Yin</a>, KTH Royal Institute of Technology, Sweden.</p>
				</div>
			</div>
		</div>


<p>&nbsp;</p>

<h3>Advisory Committee </h3>
<p><ul>
	<li><a href="https://www.kth.se/profile/dani"> Danica Kragic Jensfelt, KTH Royal Institute of Technology, Sweden. </a> </li>
	<li><a href="https://www.kth.se/profile/iolanda"> Iolanda Leite, KTH Royal Institute of Technology, Sweden. </a> </li>
	<li><a href="https://www.kth.se/profile/tumova"> Jana Tumova, KTH Royal Institute of Technology, Sweden. </a> </li>
</ul></p>

<h3>This workshop is supported by </h3>
<p><ul>
	<li><a href="https://www.ieee-ras.org/algorithms-for-planning-and-control-of-robot-motion"> IEEE-RAS Technical Committee on Algorithms for Planning and Control of Robot Motion </a>  </li>
	<li><a href="https://www.ieee-ras.org/robot-learning"> IEEE-RAS Technical Committee on Robot Learning </a> </li>
	<li><a href="https://www.ieee-ras.org/human-robot-interaction-coordination"> IEEE-RAS Technical Committee on Human-Robot Interaction and Coordination  </a> </li>
</ul></p>

</div>

</section>


<section id="rlconform21" class="wrapper style3 fade-up">

	<div class="inner">
		<h2>Previous Editions of RL-CONFORM</h2>
		For information about the workshop in 2021, visit: <a href="https://rlconform-workshop.github.io/RL-CONFORM_IROS2021">RL-CONFORM 2021</a> <br/>
	</div>

</section>

<section id="socials" class="wrapper style2 fade-up">
	<div class="inner" style="padding: 3vw 0vw 0vw 5vw;">
		<h2>Connect with us and join the conversation!</h2>

		<div class="container" style="width:30vw">
			<a class="twitter-timeline" data-dnt="true" href="https://twitter.com/RLCONFORM?ref_src=twsrc%5Etfw">Tweets by RLCONFORM</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
		</div>
	</div>
</section>


<!-- </div> -->


</section>

<!-- Footer -->
<footer id="footer" class="wrapper style1-alt">
	<div class="inner">
		<ul class="menu">
			<li>&copy; All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
		</ul>
	</div>
</footer>

<!-- Scripts -->
<script src="assets/js/jquery.min.js"></script>
<script src="assets/js/jquery.scrollex.min.js"></script>
<script src="assets/js/jquery.scrolly.min.js"></script>
<script src="assets/js/browser.min.js"></script>
<script src="assets/js/breakpoints.min.js"></script>
<script src="assets/js/util.js"></script>
<script src="assets/js/main.js"></script>


</body>
</html>
